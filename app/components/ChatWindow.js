'use client';

import { useState, useEffect, useRef } from 'react';
import { useLanguage } from '../context/LanguageContext';
import { GoogleGenAI, Modality } from '@google/genai';

// éŸ³é¢‘å¤„ç†å·¥å…·å‡½æ•°
const encode = (bytes) => {
  let binary = '';
  const len = bytes.byteLength;
  for (let i = 0; i < len; i++) {
    binary += String.fromCharCode(bytes[i]);
  }
  return btoa(binary);
};

const createBlob = (data) => {
  const l = data.length;
  const int16 = new Int16Array(l);
  for (let i = 0; i < l; i++) {
    // convert float32 -1 to 1 to int16 -32768 to 32767
    int16[i] = data[i] * 32768;
  }

  return {
    data: encode(new Uint8Array(int16.buffer)),
    mimeType: 'audio/pcm;rate=16000',
  };
};

const decode = (base64) => {
  const binaryString = atob(base64);
  const len = binaryString.length;
  const bytes = new Uint8Array(len);
  for (let i = 0; i < len; i++) {
    bytes[i] = binaryString.charCodeAt(i);
  }
  return bytes;
};

const decodeAudioData = async (data, ctx, sampleRate, numChannels) => {
  const buffer = ctx.createBuffer(
    numChannels,
    data.length / 2 / numChannels,
    sampleRate,
  );

  const dataInt16 = new Int16Array(data.buffer);
  const l = dataInt16.length;
  const dataFloat32 = new Float32Array(l);
  for (let i = 0; i < l; i++) {
    dataFloat32[i] = dataInt16[i] / 32768.0;
  }
  // Extract interleaved channels
  if (numChannels === 1) {
    buffer.copyToChannel(dataFloat32, 0);
  } else {
    for (let i = 0; i < numChannels; i++) {
      const channel = dataFloat32.filter(
        (_, index) => index % numChannels === i,
      );
      buffer.copyToChannel(channel, i);
    }
  }

  return buffer;
};

export default function ChatWindow({ article }) {
  const { nativeLanguage, targetLanguage } = useLanguage();
  const [messages, setMessages] = useState([]);
  const [userInput, setUserInput] = useState('');
  const [loading, setLoading] = useState(false);
  const [isListening, setIsListening] = useState(false);
  const [isRealtimeRecording, setIsRealtimeRecording] = useState(false);
  const isRecordingRef = useRef(false);
  const [realtimeStatus, setRealtimeStatus] = useState('');
  const [realtimeError, setRealtimeError] = useState('');
  
  const recognitionRef = useRef(null);
  const clientRef = useRef(null);
  const sessionRef = useRef(null);
  const inputAudioContextRef = useRef(null);
  const outputAudioContextRef = useRef(null);
  const inputNodeRef = useRef(null);
  const outputNodeRef = useRef(null);
  const nextStartTimeRef = useRef(0);
  const mediaStreamRef = useRef(null);
  const sourceNodeRef = useRef(null);
  const scriptProcessorNodeRef = useRef(null);
  const sourcesRef = useRef(new Set());

  // åˆå§‹åŒ–å®æ—¶éŸ³é¢‘
  useEffect(() => {
    initRealtimeAudio();
    return () => {
      // æ¸…ç†èµ„æº
      if (sessionRef.current) {
        sessionRef.current.close();
      }
      if (mediaStreamRef.current) {
        mediaStreamRef.current.getTracks().forEach(track => track.stop());
      }
    };
  }, []);

  const initRealtimeAudio = async () => {
    try {
      // åˆå§‹åŒ–éŸ³é¢‘ä¸Šä¸‹æ–‡
      inputAudioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 16000 });
      outputAudioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({ sampleRate: 24000 });
      
      inputNodeRef.current = inputAudioContextRef.current.createGain();
      outputNodeRef.current = outputAudioContextRef.current.createGain();
      outputNodeRef.current.connect(outputAudioContextRef.current.destination);
      
      nextStartTimeRef.current = outputAudioContextRef.current.currentTime;

      // åˆå§‹åŒ–Google GenAIå®¢æˆ·ç«¯
      clientRef.current = new GoogleGenAI({
        // æ³¨æ„ï¼šè¿™é‡Œéœ€è¦è®¾ç½®ä½ çš„API Keyï¼Œç”Ÿäº§ç¯å¢ƒä¸­åº”è¯¥ä»ç¯å¢ƒå˜é‡è·å–
        apiKey: process.env.NEXT_PUBLIC_GEMINI_API_KEY, // è¯·æ›¿æ¢ä¸ºä½ çš„å®é™…API Key
      });

      setRealtimeStatus('å®æ—¶éŸ³é¢‘å·²åˆå§‹åŒ–');
    } catch (error) {
      console.error('åˆå§‹åŒ–å®æ—¶éŸ³é¢‘å¤±è´¥:', error);
      setRealtimeError('åˆå§‹åŒ–å¤±è´¥: ' + error.message);
    }
  };

  const initRealtimeSession = async () => {
    const model = 'gemini-2.5-flash-preview-native-audio-dialog';

    try {
      sessionRef.current = await clientRef.current.live.connect({
        model: model,
        callbacks: {
          onopen: () => {
            setRealtimeStatus('è¿æ¥å·²å»ºç«‹');
          },
          onmessage: async (message) => {
            const audio = message.serverContent?.modelTurn?.parts[0]?.inlineData;

            if (audio) {
              nextStartTimeRef.current = Math.max(
                nextStartTimeRef.current,
                outputAudioContextRef.current.currentTime,
              );

              const audioBuffer = await decodeAudioData(
                decode(audio.data),
                outputAudioContextRef.current,
                24000,
                1,
              );
              const source = outputAudioContextRef.current.createBufferSource();
              source.buffer = audioBuffer;
              source.connect(outputNodeRef.current);
              source.addEventListener('ended', () => {
                sourcesRef.current.delete(source);
              });

              source.start(nextStartTimeRef.current);
              nextStartTimeRef.current = nextStartTimeRef.current + audioBuffer.duration;
              sourcesRef.current.add(source);
            }

            const interrupted = message.serverContent?.interrupted;
            if (interrupted) {
              for (const source of sourcesRef.current.values()) {
                source.stop();
                sourcesRef.current.delete(source);
              }
              nextStartTimeRef.current = 0;
            }
          },
          onerror: (e) => {
            console.log('åˆ›å»ºè¿æ¥æ—¶onerror', e);
            setRealtimeError(e.message);
          },
          onclose: (e) => {
            console.log('åˆ›å»ºè¿æ¥æ—¶onclose', e);
            setRealtimeStatus('è¿æ¥å…³é—­: ' + e.reason);
          },
        },
        config: {
          responseModalities: [Modality.AUDIO],
          speechConfig: {
            voiceConfig: { prebuiltVoiceConfig: { voiceName: 'Orus' } },
          },
          systemInstruction: `è§’è‰²å®šä¹‰
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è¯­è¨€å­¦ä¹ åŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©ç”¨æˆ·é€šè¿‡æ–°é—»å†…å®¹å­¦ä¹ ç›®æ ‡è¯­è¨€ã€‚ç”¨æˆ·çš„æ¯è¯­æ˜¯` + {nativeLanguage} + `ï¼Œæ­£åœ¨å­¦ä¹ ` + {targetLanguage} + `ã€‚
æ ¸å¿ƒä»»åŠ¡

æ–°é—»å†…å®¹å‘ˆç°ï¼šç”¨` + {targetLanguage} + `ä»‹ç»å’Œè®¨è®ºæ–°é—»ä¸»é¢˜åŠå†…å®¹
å®æ—¶è¯­è¨€è¾…åŠ©ï¼šè¯†åˆ«ç”¨æˆ·å¯èƒ½ä¸è®¤è¯†çš„å•è¯å’ŒçŸ­è¯­ï¼Œç”¨` + {nativeLanguage} + `è¿›è¡Œè§£é‡Š
äº’åŠ¨å­¦ä¹ ï¼šé¼“åŠ±ç”¨æˆ·ç”¨` + {targetLanguage} + `å‚ä¸è®¨è®ºï¼Œå¹¶ç»™äºˆé€‚å½“çš„è¯­è¨€æŒ‡å¯¼

å…·ä½“æ‰§è¡Œæ–¹å¼
æ–°é—»å‘ˆç°æ ¼å¼

é¦–å…ˆç”¨` + {targetLanguage} + `ç®€è¦ä»‹ç»æ–°é—»ä¸»é¢˜
ç”¨` + {targetLanguage} + `è¯¦ç»†è®²è¿°æ–°é—»å†…å®¹
åœ¨ä»‹ç»è¿‡ç¨‹ä¸­ï¼Œå¯¹å¯èƒ½çš„ç”Ÿè¯ç”¨ä»¥ä¸‹æ ¼å¼æ ‡æ³¨ï¼š
[` + {targetLanguage} + ` å•è¯/çŸ­è¯­]ï¼ˆ` + {nativeLanguage} + ` è§£é‡Šï¼‰


äº’åŠ¨æŒ‡å¯¼åŸåˆ™

è¯æ±‡è§£é‡Šï¼š

è‡ªåŠ¨è¯†åˆ«ä¸­é«˜çº§è¯æ±‡ã€ä¸“ä¸šæœ¯è¯­ã€æƒ¯ç”¨è¡¨è¾¾
ç”¨` + {nativeLanguage} + `æä¾›æ¸…æ™°ã€å‡†ç¡®çš„è§£é‡Š
å¿…è¦æ—¶æä¾›ä¾‹å¥å¸®åŠ©ç†è§£


è¯­æ³•æç¤ºï¼š

é‡åˆ°å¤æ‚è¯­æ³•ç»“æ„æ—¶ï¼Œç”¨` + {nativeLanguage} + `ç®€è¦è¯´æ˜
é‡ç‚¹å…³æ³¨` + {targetLanguage} + `çš„ç‰¹æ®Šè¯­æ³•ç°è±¡


æ–‡åŒ–èƒŒæ™¯ï¼š

æ¶‰åŠæ–‡åŒ–ç‰¹å®šæ¦‚å¿µæ—¶ï¼Œç”¨` + {nativeLanguage} + `è¡¥å……èƒŒæ™¯ä¿¡æ¯



å¯¹è¯æµç¨‹

è¯¢é—®ç”¨æˆ·æ„Ÿå…´è¶£çš„æ–°é—»ç±»å‹æˆ–è¯é¢˜
é€‰æ‹©åˆé€‚çš„æ–°é—»å†…å®¹è¿›è¡Œä»‹ç»
åœ¨ä»‹ç»è¿‡ç¨‹ä¸­å®æ—¶æä¾›è¯­è¨€è¾…åŠ©
é¼“åŠ±ç”¨æˆ·ç”¨ã€Œtarget languageã€æé—®æˆ–å‘è¡¨çœ‹æ³•
çº æ­£ç”¨æˆ·çš„è¯­è¨€é”™è¯¯ï¼Œå¹¶ç»™äºˆé¼“åŠ±æ€§åé¦ˆ

ç¤ºä¾‹å¯¹è¯ç»“æ„
AI: ä»Šå¤©æˆ‘æƒ³å’Œä½ åˆ†äº«ä¸€æ¡å…³äº` + {article} + `çš„æ¶ˆæ¯ã€‚

[ç”¨` + {targetLanguage} + `ä»‹ç»æ–°é—»ï¼Œå…³é”®è¯æ±‡ç”¨` + {nativeLanguage} + `æ ‡æ³¨]

ä½ å¯¹è¿™ä¸ªè¯é¢˜æœ‰ä»€ä¹ˆçœ‹æ³•å—ï¼Ÿè¯·å°è¯•ç”¨` + {targetLanguage} + `æ¥è¡¨è¾¾ã€‚

ç”¨æˆ·: [ç”¨æˆ·å›åº”]

AI: [ç”¨` + {targetLanguage} + `å›åº”ï¼ŒåŒæ—¶çº æ­£è¯­è¨€é”™è¯¯ï¼Œé¼“åŠ±ç»§ç»­å¯¹è¯]
æ³¨æ„äº‹é¡¹

ä¿æŒè€å¿ƒå’Œé¼“åŠ±çš„æ€åº¦
æ ¹æ®ç”¨æˆ·çš„è¯­è¨€æ°´å¹³è°ƒæ•´éš¾åº¦
å¹³è¡¡è¯­è¨€å­¦ä¹ å’Œæ–°é—»å†…å®¹çš„æ¯”é‡
è¥é€ è½»æ¾æ„‰å¿«çš„å­¦ä¹ æ°›å›´
é€‚æ—¶è¯¢é—®ç”¨æˆ·æ˜¯å¦éœ€è¦æ›´å¤šè§£é‡Šæˆ–æƒ³è®¨è®ºå…¶ä»–è¯é¢˜

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å¼€å§‹ä»Šå¤©çš„æ–°é—»è¯­è¨€å­¦ä¹ ä¹‹æ—…å§ï¼ä½ å¸Œæœ›äº†è§£å“ªä¸ªé¢†åŸŸçš„æ–°é—»å‘¢ï¼Ÿ`,
        },
      });
    } catch (e) {
      console.log('åˆ›å»ºä¼šè¯å¤±è´¥:', e);
      setRealtimeError('åˆ›å»ºä¼šè¯å¤±è´¥: ' + e.message);
    }
  };

  const startRealtimeRecording = async () => {
    if (isRealtimeRecording) {
      return;
    }

    if (!sessionRef.current) {
      await initRealtimeSession();
    }

    inputAudioContextRef.current.resume();
    outputAudioContextRef.current.resume();
    setRealtimeStatus('è¯·æ±‚éº¦å…‹é£æƒé™...');

    try {
      mediaStreamRef.current = await navigator.mediaDevices.getUserMedia({
        audio: true,
        video: false,
      });

      setRealtimeStatus('éº¦å…‹é£æƒé™å·²æˆäºˆï¼Œå¼€å§‹æ•è·...');

      sourceNodeRef.current = inputAudioContextRef.current.createMediaStreamSource(mediaStreamRef.current);
      sourceNodeRef.current.connect(inputNodeRef.current);

      const bufferSize = 256;
      scriptProcessorNodeRef.current = inputAudioContextRef.current.createScriptProcessor(bufferSize, 1, 1);

      scriptProcessorNodeRef.current.onaudioprocess = (audioProcessingEvent) => {
        if (!isRecordingRef.current) return;

        const inputBuffer = audioProcessingEvent.inputBuffer;
        const pcmData = inputBuffer.getChannelData(0);

        if (sessionRef.current) {
          sessionRef.current.sendRealtimeInput({ media: createBlob(pcmData) });
        }
      };

      sourceNodeRef.current.connect(scriptProcessorNodeRef.current);
      scriptProcessorNodeRef.current.connect(inputAudioContextRef.current.destination);

      isRecordingRef.current = true;
      setIsRealtimeRecording(true);
      setRealtimeStatus('ğŸ”´ æ­£åœ¨å½•éŸ³... å®æ—¶å¯¹è¯ä¸­');
    } catch (err) {
      console.error('å¼€å§‹å½•éŸ³æ—¶å‡ºé”™:', err);
      setRealtimeStatus(`é”™è¯¯: ${err.message}`);
      stopRealtimeRecording();
    }
  };

  const stopRealtimeRecording = () => {
    if (!isRealtimeRecording && !mediaStreamRef.current && !inputAudioContextRef.current) {
      return;
    }

    setRealtimeStatus('åœæ­¢å½•éŸ³...');
    isRecordingRef.current = false;
    setIsRealtimeRecording(false);

    if (scriptProcessorNodeRef.current && sourceNodeRef.current && inputAudioContextRef.current) {
      scriptProcessorNodeRef.current.disconnect();
      sourceNodeRef.current.disconnect();
    }

    scriptProcessorNodeRef.current = null;
    sourceNodeRef.current = null;

    if (mediaStreamRef.current) {
      mediaStreamRef.current.getTracks().forEach((track) => track.stop());
      mediaStreamRef.current = null;
    }

    setRealtimeStatus('å½•éŸ³å·²åœæ­¢ï¼Œç‚¹å‡»å¼€å§‹è¿›è¡Œæ–°çš„å¯¹è¯');
  };

  const resetRealtimeSession = () => {
    sessionRef.current?.close();
    initRealtimeSession();
    setRealtimeStatus('ä¼šè¯å·²é‡ç½®');
  };

  // åŸæœ‰çš„æ–‡ç« åˆå§‹åŒ–èŠå¤©åŠŸèƒ½
  useEffect(() => {
    if (article) {
      const initChat = async () => {
        setLoading(true);
        setMessages([]);
        try {
          const response = await fetch('/api/chat', {
            method: 'POST',
            headers: { 'Content-Type': 'application/json' },
            body: JSON.stringify({ article, targetLang: targetLanguage }),
          });
          const data = await response.json();
          const aiMessage = `${data.summary}\n\nHere are some questions for you:\n1. ${data.questions[0]}\n2. ${data.questions[1]}\n3. ${data.questions[2]}`;
          setMessages([{ sender: 'ai', text: aiMessage }]);
        } catch (error) {
          setMessages([{ sender: 'ai', text: 'Sorry, I had trouble starting the conversation.' }]);
        } finally {
          setLoading(false);
        }
      };
      initChat();
    }
  }, [article, targetLanguage]);

  const handlePlayAudio = (text) => {
    if ('speechSynthesis' in window) {
      window.speechSynthesis.cancel();
      const utterance = new SpeechSynthesisUtterance(text);
      window.speechSynthesis.speak(utterance);
    } else {
      alert("Sorry, your browser does not support text-to-speech.");
    }
  };

  const handleListen = () => {
    if (!('webkitSpeechRecognition' in window)) {
      alert("Sorry, your browser does not support speech recognition.");
      return;
    }

    if (isListening) {
      recognitionRef.current.stop();
      setIsListening(false);
      return;
    }

    const recognition = new window.webkitSpeechRecognition();
    recognition.continuous = false;
    recognition.interimResults = false;

    recognition.onstart = () => setIsListening(true);
    recognition.onend = () => setIsListening(false);
    recognition.onerror = () => setIsListening(false);

    recognition.onresult = (event) => {
      const transcript = event.results[0][0].transcript;
      setUserInput(transcript);
    };

    recognition.start();
    recognitionRef.current = recognition;
  };

  return (
    <div className="h-1/2 bg-white p-4 rounded-lg flex flex-col shadow">
      <h2 className="text-xl font-bold mb-4 text-gray-800">AI èŠå¤©</h2>
      
      {/* å®æ—¶å¯¹è¯æ§åˆ¶åŒºåŸŸ */}
      <div className="mb-4 p-3 bg-gray-100 rounded-lg">
        <h3 className="text-lg font-semibold mb-2 text-gray-700">å®æ—¶è¯­éŸ³å¯¹è¯</h3>
        <div className="flex gap-2 mb-2">
          <button
            onClick={resetRealtimeSession}
            disabled={isRealtimeRecording}
            className="px-3 py-1 bg-gray-500 text-white rounded hover:bg-gray-600 disabled:opacity-50 text-sm"
          >
            é‡ç½®ä¼šè¯
          </button>
          <button
            onClick={startRealtimeRecording}
            disabled={isRealtimeRecording}
            className="px-3 py-1 bg-green-500 text-white rounded hover:bg-green-600 disabled:opacity-50 text-sm"
          >
            å¼€å§‹å½•éŸ³
          </button>
          <button
            onClick={stopRealtimeRecording}
            disabled={!isRealtimeRecording}
            className="px-3 py-1 bg-red-500 text-white rounded hover:bg-red-600 disabled:opacity-50 text-sm"
          >
            åœæ­¢å½•éŸ³
          </button>
        </div>
        <div className="text-sm text-gray-600">
          çŠ¶æ€: {realtimeStatus}
          {realtimeError && <div className="text-red-500">é”™è¯¯: {realtimeError}</div>}
        </div>
      </div>

      <div className="flex-grow overflow-y-auto mb-4 p-4 bg-gray-50 rounded-lg">
        {loading && <p>AI is thinking...</p>}
        {!article && !loading && <p className="text-gray-500">Select an article to start a conversation.</p>}
        {messages.map((msg, index) => (
          <div key={index} className={`mb-3 p-3 rounded-lg max-w-xl ${msg.sender === 'ai' ? 'bg-blue-100 text-blue-900' : 'bg-green-100 text-green-900 ml-auto'}`}>
            {msg.text.split('\n').map((line, i) => <p key={i}>{line}</p>)}
            {msg.sender === 'ai' && (
              <button onClick={() => handlePlayAudio(msg.text)} className="mt-2 text-blue-600">
                <svg xmlns="http://www.w3.org/2000/svg" className="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                  <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M14.752 11.168l-3.197-2.132A1 1 0 0010 9.87v4.263a1 1 0 001.555.832l3.197-2.132a1 1 0 000-1.664z" />
                  <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M21 12a9 9 0 11-18 0 9 9 0 0118 0z" />
                </svg>
              </button>
            )}
          </div>
        ))}
      </div>
      <div className="flex items-center">
        <input 
          type="text" 
          className="flex-grow border rounded-l-lg p-2 focus:outline-none focus:ring-2 focus:ring-blue-500" 
          placeholder="Type your message..."
          value={userInput}
          onChange={(e) => setUserInput(e.target.value)}
        />
        <button onClick={handleListen} className={`p-2 border-y ${isListening ? 'bg-red-500 text-white' : 'bg-gray-100'}`}>
          <svg xmlns="http://www.w3.org/2000/svg" className="h-6 w-6" fill="none" viewBox="0 0 24 24" stroke="currentColor">
            <path strokeLinecap="round" strokeLinejoin="round" strokeWidth={2} d="M19 11a7 7 0 01-7 7m0 0a7 7 0 01-7-7m7 7v4m0 0H8m4 0h4m-4-8a3 3 0 01-3-3V5a3 3 0 116 0v6a3 3 0 01-3 3z" />
          </svg>
        </button>
        <button className="bg-blue-600 text-white px-6 py-2 rounded-r-lg hover:bg-blue-700 transition-colors h-full">Send</button>
      </div>
    </div>
  );
}
